{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FacePhi Challenge Baseline",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FacePhi Challenge 2022"
      ],
      "metadata": {
        "id": "ZCkU9zYFlgZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "0hqbw-oQllzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading and extraction"
      ],
      "metadata": {
        "id": "69DlEeYHlaxX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPcotMANm2GC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tqdm\n",
        "import argparse\n",
        "from urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "midv500_links = [\n",
        "    \"ftp://smartengines.com/midv-500/dataset/01_alb_id.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/dataset/05_aze_passport.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/dataset/21_esp_id_old.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/dataset/22_est_id.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/dataset/24_fin_id.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/dataset/25_grc_passport.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/dataset/32_lva_passport.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/dataset/39_rus_internalpassport.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/dataset/41_srb_passport.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/dataset/42_svk_id.zip\",\n",
        "]\n",
        "\n",
        "midv2019_links = [\n",
        "    \"ftp://smartengines.com/midv-500/extra/midv-2019/dataset/01_alb_id.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/extra/midv-2019/dataset/05_aze_passport.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/extra/midv-2019/dataset/21_esp_id_old.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/extra/midv-2019/dataset/22_est_id.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/extra/midv-2019/dataset/24_fin_id.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/extra/midv-2019/dataset/25_grc_passport.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/extra/midv-2019/dataset/32_lva_passport.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/extra/midv-2019/dataset/39_rus_internalpassport.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/extra/midv-2019/dataset/41_srb_passport.zip\",\n",
        "    \"ftp://smartengines.com/midv-500/extra/midv-2019/dataset/42_svk_id.zip\",\n",
        "]\n",
        "\n",
        "midv2020_links = [\"ftp://smartengines.com//midv-2020/dataset/photo.tar\"]\n",
        "\n",
        "def extract(path):\n",
        "    out_path, extension = os.path.splitext(path)\n",
        "\n",
        "    if extension == \".tar\":\n",
        "        with tarfile.open(path, \"r:\") as tar:\n",
        "            tar.extractall(out_path, )\n",
        "    elif extension == \".zip\":\n",
        "        with zipfile.ZipFile(path) as zf:\n",
        "            zf.extractall(out_path)\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "class tqdm_upto(tqdm.tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "def download(url: str, save_dir: str):\n",
        "    # Creates save_dir if it does not exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Downloads the file\n",
        "    with tqdm_upto(unit=\"B\", unit_scale=True, miniters=1) as t: \n",
        "        urlretrieve(\n",
        "            url,\n",
        "            filename=os.path.join(save_dir, url.split(\"/\")[-1]),\n",
        "            reporthook=t.update_to,\n",
        "            data=None,\n",
        "        )\n",
        "\n",
        "def download_and_extract(links_set, download_dir: str = './data'):\n",
        "    out_path = os.path.join(download_dir)\n",
        "    for i, link in enumerate(links_set):\n",
        "        # download zip file\n",
        "        link = link.replace(\"\\\\\", \"/\")\n",
        "        filename = os.path.basename(link)\n",
        "        print()\n",
        "        print(f\"Downloading {i+1}/{len(links_set)}:\", filename)\n",
        "        download(link, out_path)\n",
        "\n",
        "        # unzip zip file\n",
        "        print(\"Unzipping:\", filename)\n",
        "        zip_path = os.path.join(out_path, filename)\n",
        "        extract(zip_path)\n",
        "\n",
        "        # remove zip file\n",
        "        os.remove(zip_path)\n",
        "\n",
        "download_and_extract(midv500_links, download_dir='data/midv500')\n",
        "# download_and_extract(midv2019_links, download_dir='data/midv2019')\n",
        "# download_and_extract(midv2020_links, download_dir='data/midv2020')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "import glob\n",
        "import tqdm\n",
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import patches\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 16)\n",
        "\n",
        "\n",
        "classes = [\n",
        "    \"alb_id\",\n",
        "    \"aze_passport\",\n",
        "    \"esp_id\",\n",
        "    \"est_id\",\n",
        "    \"fin_id\",\n",
        "    \"grc_passport\",\n",
        "    \"lva_passport\",\n",
        "    \"rus_internalpassport\",\n",
        "    \"srb_passport\",\n",
        "    \"svk_id\",\n",
        "]\n",
        "\n",
        "def get_class(*, img_path: str, dataset: str):\n",
        "    if dataset in ['midv500', 'midv2019']:\n",
        "        dirname = img_path.split('/')[-4]\n",
        "        return '_'.join(dirname.split('_')[1:3])\n",
        "    else:\n",
        "        dirname = img_path.split('/')[-2]\n",
        "        return '_'.join(dirname.split('_')[:2])\n",
        "\n",
        "def get_location(*, loc_path: str):\n",
        "    return json.load(open(loc_path, 'r'))['quad']\n",
        "\n",
        "def get_location_path(img_path: str, loc_dirname: str = 'ground_truth'):\n",
        "    loc_path = img_path.replace('images', loc_dirname)\n",
        "    loc_path = os.path.splitext(loc_path)[0]+'.json'\n",
        "    return loc_path\n",
        "\n",
        "def get_metadata(image_paths: List[str], dataset: str, gt_dirname: str = 'ground_truth'):\n",
        "    location_paths = [get_location_path(img_path=path, loc_dirname=gt_dirname) for path in image_paths]\n",
        "    locations = np.stack(\n",
        "        [get_location(loc_path=path) for path in location_paths],\n",
        "        axis=0\n",
        "    )\n",
        "    class_labels = np.array([get_class(img_path=path, dataset=dataset) for path in image_paths])\n",
        "\n",
        "    return locations, class_labels\n",
        "\n",
        "def get_midv500_data(path='data/midv500'):\n",
        "    image_paths = glob.glob(os.path.join(path, '*', '*', 'images', '*', '*'))\n",
        "    locations, class_labels = get_metadata(image_paths, dataset='midv500')\n",
        "\n",
        "    return image_paths, locations, class_labels\n",
        "\n",
        "def get_midv2019_data(path='data/midv2019'):\n",
        "    image_paths = glob.glob(os.path.join(path, '*', 'images', '*', '*'))\n",
        "    locations, class_labels = get_metadata(image_paths, dataset='midv2019')\n",
        "\n",
        "    return image_paths, locations, class_labels\n",
        "\n",
        "def get_midv2020_data(path='data/midv2020'):\n",
        "    gt_paths = glob.glob(os.path.join(path, 'photo', 'annotations', '*.json'))\n",
        "\n",
        "    class_labels = []\n",
        "    locations = []\n",
        "    image_paths = []\n",
        "\n",
        "    for gt_path in gt_paths:\n",
        "        json_data = json.load(open(gt_path, 'r'))\n",
        "        # class_name = json_data['_via_settings']['project']['name']\n",
        "        class_name = os.path.splitext(os.path.basename(gt_path))[0]\n",
        "        basedir = os.path.join(path, 'photo', 'images', class_name)\n",
        "\n",
        "        for k, v in json_data['_via_img_metadata'].items():\n",
        "            image_paths.append(os.path.join(basedir, v['filename']))\n",
        "            for reg in v['regions']:\n",
        "                if reg['shape_attributes']['name'] == 'polygon':\n",
        "                    x = reg['shape_attributes']['all_points_x']\n",
        "                    y = reg['shape_attributes']['all_points_y']\n",
        "                    loc = np.stack([x, y], axis=1)\n",
        "            locations.append(loc)\n",
        "            class_labels.append(class_name)\n",
        "\n",
        "    return image_paths, locations, class_labels\n",
        "\n",
        "def show_image(image_path: str, location: np.ndarray = None, label: str = None):\n",
        "    plt.imshow(load_img(image_path))\n",
        "\n",
        "    if location is not None:\n",
        "        x, y = location[:, 0], location[:, 1]\n",
        "        plt.plot(np.append(x, x[0]), np.append(y, y[0]), color=(1, 0, 0), linewidth=2.0)\n",
        "    if label is not None:\n",
        "        print(label)\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oHiQRW_Yq_az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "BYw3XFV2lu2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_img(path: str, size: Tuple[int, int] = None):\n",
        "    img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
        "    if size is not None:\n",
        "        img = cv2.resize(img, size)\n",
        "    return img\n",
        "\n",
        "def preprocess(\n",
        "    image_paths: List[str],\n",
        "    locations: np.ndarray,\n",
        "    labels: List[str],\n",
        "    image_size: Tuple[int, int],\n",
        "    class_names: str = None\n",
        "):\n",
        "    if class_names is None:\n",
        "        unique_labels, label_ids = np.unique(labels, return_inverse=True)\n",
        "    else:\n",
        "        unique_labels = class_names\n",
        "        label_ids = np.array([class_names.index(l) for l in labels])\n",
        "\n",
        "    images = np.zeros((len(image_paths), image_size[0], image_size[1], 3), dtype=np.uint8)\n",
        "    for i, path in enumerate(tqdm.tqdm(image_paths)):\n",
        "        images[i] = load_img(path, size=image_size)\n",
        "\n",
        "    # Normalize in range (-1, 1)\n",
        "    images = images.astype(np.float32) / 127.5 - 1.0\n",
        "    return images, label_ids, unique_labels\n",
        "\n",
        "\n",
        "input_size = (224, 224)\n",
        "image_paths, locations, labels = get_midv500_data()\n",
        "train_images, train_label_ids, unique_labels = preprocess(image_paths, locations, labels, input_size)"
      ],
      "metadata": {
        "id": "kZysiNELrhCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization"
      ],
      "metadata": {
        "id": "6iBigoNvl0Jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
        "\n",
        "idx = np.random.randint(len(train_images))\n",
        "img = load_img(image_paths[idx], size=(224, 224))\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j9q9pEpGdhbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimentation"
      ],
      "metadata": {
        "id": "FJClvDVWl725"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model definition"
      ],
      "metadata": {
        "id": "BSTooQIJmBXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def get_model(input_shape: Tuple[int, int, int], depth: int = 4, n_classes: int = 10):\n",
        "    inp = x = tf.keras.layers.Input(input_shape)\n",
        "\n",
        "    for d in range(depth):\n",
        "        x = tf.keras.layers.Conv2D(2**(5+d), 3, padding='same', activation='relu')(x)\n",
        "        x = tf.keras.layers.MaxPooling2D()(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dense(n_classes)(x)\n",
        "\n",
        "    return tf.keras.models.Model(inputs=inp, outputs=x)\n",
        "\n",
        "model = get_model(input_shape=input_size+(3,), depth=4, n_classes=len(unique_labels))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "UfVRt_i2-FGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "vuVGJ_b5mE03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=['acc'])\n",
        "\n",
        "\n",
        "model.fit(train_images, train_label_ids, batch_size=64, epochs=10, shuffle=True)"
      ],
      "metadata": {
        "id": "dSw7d71eA9Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "5f6UWK6OmHW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download_and_extract(midv2020_links, download_dir='data/midv2020')\n",
        "\n",
        "test_image_paths, test_locations, test_labels = get_midv2020_data()\n",
        "\n",
        "test_images, test_label_ids, _ = preprocess(test_image_paths, test_locations, test_labels, input_size)"
      ],
      "metadata": {
        "id": "FGA8GPWXELHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(test_images, batch_size=32, verbose=1)\n",
        "preds = np.argmax(preds, axis=-1)"
      ],
      "metadata": {
        "id": "iprH-GTiYV-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submission"
      ],
      "metadata": {
        "id": "beUbNqO6mMSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "print(\"MacroF1:\", f1_score(y_true=test_label_ids, y_pred=preds, labels=np.arange(10), average='macro'))\n",
        "print(\"MicroF1:\", f1_score(y_true=test_label_ids, y_pred=preds, labels=np.arange(10), average='micro'))"
      ],
      "metadata": {
        "id": "rdwSWXrdYZ25"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}